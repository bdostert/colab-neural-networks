{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch #Pytorch is a Python module that can create neural networks  and automatically do backpropogation for training a network.\n",
        "import torch.nn as nn #Torch.nn is a submodule of torch that can create various types of networks and functions that operate on them.\n",
        "\n",
        "\n",
        "# single-direction RNN, optionally tied embeddings\n",
        "class Emb_RNN(nn.Module):\n",
        "    def __init__(self, params, use_LSTM=False):\n",
        "        super(Emb_RNN, self).__init__()\n",
        "        self.d_embs = params['d_emb'] #dimension of embeddings\n",
        "        self.d_hid =  params['d_hid'] #dimension of hidden layer\n",
        "        self.embeddings= nn.Embedding(params['num_chs'], self.d_embs) #A separate embeddding for each character in char list\n",
        "        self.use_LSTM = use_LSTM #LSTM is more powerful than a simple RNN\n",
        "        # input to recurrent layer, default nonlinearity is tanh\n",
        "        if use_LSTM:\n",
        "            self.i2R = nn.LSTMCell(self.d_embs, self.d_hid)\n",
        "        else:\n",
        "            self.i2R = nn.RNNCell(self.d_embs, self.d_hid)\n",
        "        # recurrent to output layer\n",
        "        self.R2o = nn.Linear(self.d_hid, params['num_chs'])\n",
        "        if self.d_embs == self.d_hid:\n",
        "            self.R2o.weight = self.embeddings.weight\n",
        "\n",
        "\n",
        "    def forward(self, ch_indices):\n",
        "        preds = [] #initialize list of predictions, each of which is a score for each character\n",
        "        for j, ch_ix in enumerate(ch_indices):\n",
        "            emb = self.embeddings(ch_ix) #Get the embedding of the character\n",
        "            emb = torch.unsqueeze(emb, 0)\n",
        "            if self.use_LSTM:\n",
        "                if j == 0:\n",
        "                    hidden, context = self.i2R(emb) #We don't supply the hidden or context the first time.\n",
        "                                                     #Pytorch will default it to zeroes.\n",
        "                else:\n",
        "                    hidden, context = self.i2R(emb, (hidden, context))\n",
        "            else:\n",
        "                if j == 0:\n",
        "                    hidden = self.i2R(emb)\n",
        "                else:\n",
        "                    hidden = self.i2R(emb, hidden)\n",
        "            preds.append(self.R2o(hidden))\n",
        "        return torch.stack(preds, dim=1) #The predictions of the characters are stacked into one matrix. Each row is a prediction set.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3Y6PkBlQMrnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyLeDXpxL51N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import re\n",
        "import sys\n",
        "import collections\n",
        "import os\n",
        "import random\n",
        "\n",
        "verbose = False\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "d_emb = 64 #Hyperparameters can be changed here.\n",
        "n_layers = 1\n",
        "d_hid = 64\n",
        "lr = 0.003\n",
        "use_LSTM = True\n",
        "if use_LSTM:\n",
        "    model_type = 'lstm'\n",
        "else:\n",
        "    model_type = 'rnn'\n",
        "\n",
        "\n",
        "def train(net, words, params):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimiser = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    if os.path.exists(params['save_path']):\n",
        "        checkpoint = torch.load(params['save_path'])\n",
        "        print('Loading checkpoint')\n",
        "        net.load_state_dict(checkpoint['net_state_dict'])\n",
        "        optimiser.load_state_dict(checkpoint['optimiser_state_dict'])\n",
        "        net.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        ep_loss = 0.\n",
        "        num_tested = 0\n",
        "        num_correct = 0\n",
        "        for counter, i in enumerate(torch.randperm(len(words))): #Randonly choose a word.\n",
        "            pred = net(words[i]) #Predict on the model.\n",
        "            pred = pred[:,:-1,:].contiguous().view(-1, pred.size(-1)) #Offset the predictions from the target\n",
        "                    #so that we predict the next character based on the previous character.\n",
        "            target = words[i][1:]\n",
        "            target = target.contiguous().view(-1)\n",
        "            target = target.long()\n",
        "            #print('pt', pred.size(), target.size())\n",
        "            with torch.no_grad():\n",
        "                pred_numpy = np.argmax(pred.numpy(), axis=1).tolist()\n",
        "                target_numpy = target.numpy().tolist()\n",
        "                matched_chars = [c1 for c1, c2 in zip(pred_numpy, target_numpy) if c1 == c2]\n",
        "                num_tested += len(target_numpy)\n",
        "                num_correct += len(matched_chars)\n",
        "                if counter % 1000 == 0:\n",
        "                    #print(''.join([ix2ch[str(c)] for c in pred_numpy]), ''.join([ix2ch[str(c)] for c in target_numpy]))\n",
        "                    for k in range(len(target_numpy)):\n",
        "                        print(''.join([ix2ch[str(c)] for c in target_numpy[:k]]), ''.join([ix2ch[str(c)] for c in target_numpy[:k]])+ix2ch[str(pred_numpy[k])])\n",
        "                    print()\n",
        "            loss = criterion(pred, target)\n",
        "            if torch.isnan(loss):\n",
        "                with torch.no_grad():\n",
        "                    print(pred, target, words[i], ix2ch[i])\n",
        "                    exit()\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            optimiser.zero_grad()\n",
        "            ep_loss += loss.detach()\n",
        "        print('Epoch', epoch, 'Accuracy', round(num_correct / num_tested, 4), 'Loss', ep_loss)\n",
        "        print('Saving checkpoint')\n",
        "        torch.save({'net_state_dict': net.state_dict(),  'optimiser_state_dict': optimiser.state_dict()}, params['save_path'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "words = collections.defaultdict(lambda: [])\n",
        "words_as_indices = {}\n",
        "models = {} #Put the models in a dictionary in case we train on multiple models for multiple datasets.\n",
        "path = '' #No directory needs to be specified if the files are uploaded here.\n",
        "#word_files = ['english']\n",
        "word_files = ['ukwords.txt']\n",
        "input('Make sure that the data files are uploaded to Colab for this session \\nand then press Enter to continue.')\n",
        "for word_file in word_files:\n",
        "    chars = []\n",
        "    print(path+word_file)\n",
        "    if os.path.isfile(path+word_file):\n",
        "        print('Processing file', word_file)\n",
        "        with open(path+word_file, 'r') as f0:\n",
        "            for i, line in enumerate(f0.readlines()):\n",
        "                if i % 1000 == 0:\n",
        "                    print('Processed', i, 'lines.')\n",
        "                line = line.rstrip()\n",
        "                line = '#' + line + '#'\n",
        "                if len(line) < 4:\n",
        "                    continue\n",
        "                words[word_file].append(line)\n",
        "                for ch in line:\n",
        "                     if ch not in chars:\n",
        "                         chars.append(ch.lower())\n",
        "    else:\n",
        "        print('No file found with  name', word_file)\n",
        "        exit()\n",
        "\n",
        "    ch2ix = {}\n",
        "    ix2ch = {}\n",
        "    total_chars = len(chars)\n",
        "    print('total chars', total_chars)\n",
        "    for i, char in enumerate(chars):\n",
        "        ch2ix[char] = i #Set up dictionaries for converting characters to indices and vice versa.\n",
        "        ix2ch[str(i)] = char\n",
        "    with open(word_file+'.ch2ix.json', 'w') as f1:\n",
        "        json.dump(ch2ix, f1)\n",
        "    with open(word_file+'.ix2ch.json', 'w') as f2:\n",
        "        json.dump(ix2ch, f2)\n",
        "    words_as_indices[word_file] = [torch.LongTensor([ch2ix[c] for c in word])\n",
        "        for word in words[word_file]\n",
        "      ]\n",
        "\n",
        "    params = {'num_chs': total_chars,\n",
        "              'd_emb': d_emb,\n",
        "              'num_layers': n_layers,\n",
        "              'd_hid': d_hid,\n",
        "              'lr': lr,\n",
        "              'epochs': num_epochs,\n",
        "              'save_path': word_file+'.'+model_type+'.d_emb'+str(d_emb)+'.n_layers'+str(n_layers)+'.d_hid'+str(d_hid)+'.lr'+str(lr)+'.pth'}\n",
        "\n",
        "\n",
        "    models[word_file] = Emb_RNN(params, use_LSTM)\n",
        "    train(models[word_file], words_as_indices[word_file], params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GETnl7KZMOhJ",
        "outputId": "2a73cdd2-203b-419a-e05d-57676c40b628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Make sure that the data files are uploaded to Colab for this session \n",
            "and then press Enter to continue.\n",
            "ukwords.txt\n",
            "Processing file ukwords.txt\n",
            "Processed 0 lines.\n",
            "Processed 1000 lines.\n",
            "Processed 2000 lines.\n",
            "Processed 3000 lines.\n",
            "Processed 4000 lines.\n",
            "Processed 5000 lines.\n",
            "Processed 6000 lines.\n",
            "Processed 7000 lines.\n",
            "Processed 8000 lines.\n",
            "Processed 9000 lines.\n",
            "Processed 10000 lines.\n",
            "Processed 11000 lines.\n",
            "Processed 12000 lines.\n",
            "total chars 27\n",
            "Loading checkpoint\n",
            " c\n",
            "d de\n",
            "de des\n",
            "des dest\n",
            "desp despe\n",
            "despi despic\n",
            "despis despist\n",
            "despise despise#\n",
            "\n",
            " s\n",
            "o op\n",
            "op opt\n",
            "opp oppo\n",
            "oppr oppro\n",
            "oppre oppres\n",
            "oppres oppress\n",
            "oppress oppress#\n",
            "oppressi oppressio\n",
            "oppressin oppressing\n",
            "oppressing oppressing#\n",
            "\n",
            " s\n",
            "a an\n",
            "am amb\n",
            "ama amar\n",
            "amaz amaze\n",
            "amaze amazes\n",
            "\n",
            " s\n",
            "c co\n",
            "co con\n",
            "com comp\n",
            "comp compa\n",
            "compl comple\n",
            "compla complai\n",
            "complac complaci\n",
            "complace complaces\n",
            "complacen complacent\n",
            "complacent complacent#\n",
            "\n",
            " s\n",
            "a al\n",
            "as ass\n",
            "ast astr\n",
            "asto astor\n",
            "aston astone\n",
            "astoni astonic\n",
            "astonis astonist\n",
            "astonish astonishe\n",
            "astonishi astonishin\n",
            "astonishin astonishing\n",
            "astonishing astonishing#\n",
            "\n",
            " s\n",
            "h ha\n",
            "ho hor\n",
            "hom home\n",
            "homo homou\n",
            "homos homos#\n",
            "homose homosed\n",
            "homosex homosex#\n",
            "homosexu homosexua\n",
            "homosexua homosexuat\n",
            "homosexual homosexual#\n",
            "homosexuali homosexualit\n",
            "homosexualit homosexuality\n",
            "homosexuality homosexuality#\n",
            "\n",
            " c\n",
            "c co\n",
            "ca car\n",
            "cap cape\n",
            "capa capat\n",
            "capac capacy\n",
            "capaci capacit\n",
            "capacit capacity\n",
            "capacity capacity#\n",
            "\n",
            " s\n",
            "i in\n",
            "id ide\n",
            "ide iden\n",
            "iden ident\n",
            "ident ident#\n",
            "identi identia\n",
            "identic identic#\n",
            "identica identical\n",
            "identical identical#\n",
            "identicall identically\n",
            "identically identically#\n",
            "\n",
            " c\n",
            "a an\n",
            "al all\n",
            "ale ales\n",
            "aler alera\n",
            "alert alerte\n",
            "\n",
            " s\n",
            "f fa\n",
            "fl fla\n",
            "fle flea\n",
            "flex flex#\n",
            "flexi flexie\n",
            "flexib flexibl\n",
            "flexibl flexible\n",
            "flexible flexible#\n",
            "\n",
            " s\n",
            "b ba\n",
            "ba bar\n",
            "bat batt\n",
            "batt batte\n",
            "battl battle\n",
            "battle battle#\n",
            "\n",
            " s\n",
            "f fo\n",
            "fl flo\n",
            "flo flow\n",
            "flow flow#\n",
            "\n",
            " s\n",
            "d de\n",
            "de des\n",
            "den dent\n",
            "\n",
            "Epoch 0 Accuracy 0.4049 Loss tensor(23391.2832)\n",
            "Saving checkpoint\n",
            " c\n",
            "n ne\n",
            "na nam\n",
            "nat nate\n",
            "natu natua\n",
            "natur nature\n",
            "natura naturat\n",
            "natural natural#\n",
            "naturall naturally\n",
            "naturally naturally#\n",
            "\n",
            " s\n",
            "s st\n",
            "so sol\n",
            "sof soff\n",
            "soft soft#\n",
            "softw softwo\n",
            "softwa softwar\n",
            "softwar softward\n",
            "software software#\n",
            "\n",
            " s\n",
            "s st\n",
            "sa sav\n",
            "sat sate\n",
            "satu satur\n",
            "satur sature\n",
            "satura satural\n",
            "saturat saturati\n",
            "saturate saturate#\n",
            "saturated saturated#\n",
            "\n",
            " s\n",
            "a ac\n",
            "ap app\n",
            "apo apor\n",
            "apoc apoce\n",
            "apocr apocre\n",
            "apocry apocry#\n",
            "apocryp apocrype\n",
            "apocryph apocryphe\n",
            "apocrypha apocryphat\n",
            "apocryphal apocryphall\n",
            "\n",
            " s\n",
            "l li\n",
            "lo low\n",
            "loa load\n",
            "loat loat#\n",
            "loath loath#\n",
            "loaths loaths#\n",
            "loathso loathsom\n",
            "loathsom loathsome\n",
            "loathsome loathsome#\n",
            "\n",
            " s\n",
            "l lo\n",
            "la lan\n",
            "lac lack\n",
            "lack lack#\n",
            "\n",
            " s\n",
            "e ex\n",
            "ej eje\n",
            "eje ejec\n",
            "ejec eject\n",
            "eject ejecti\n",
            "\n",
            " s\n",
            "a ac\n",
            "an ant\n",
            "ant anth\n",
            "anti antim\n",
            "antic antic#\n",
            "antics antics#\n",
            "\n",
            " s\n",
            "m ma\n",
            "ma man\n",
            "man mani\n",
            "mani manis\n",
            "manif manifi\n",
            "manife manifer\n",
            "manifes manifes#\n",
            "manifest manifest#\n",
            "manifests manifests#\n",
            "\n",
            " s\n",
            "s st\n",
            "st sta\n",
            "sta star\n",
            "stak stake\n",
            "stake stake#\n",
            "stakes stakes#\n",
            "\n",
            " s\n",
            "v vi\n",
            "vo vor\n",
            "vot vote\n",
            "voti votic\n",
            "votin voting\n",
            "voting voting#\n",
            "\n",
            " s\n",
            "b be\n",
            "br bro\n",
            "bre brea\n",
            "brea break\n",
            "breat breat#\n",
            "breath breath#\n",
            "breathi breathin\n",
            "breathin breathing\n",
            "breathing breathing#\n",
            "\n",
            " s\n",
            "t tr\n",
            "th tho\n",
            "thr thro\n",
            "thri thrin\n",
            "thril thrill\n",
            "thrill thrill#\n",
            "thrilli thrillin\n",
            "thrillin thrilling\n",
            "thrilling thrilling#\n",
            "\n",
            "Epoch 1 Accuracy 0.4084 Loss tensor(23265.5703)\n",
            "Saving checkpoint\n",
            " s\n",
            "s su\n",
            "su sup\n",
            "sur surg\n",
            "surr surre\n",
            "surro surron\n",
            "surrou surroun\n",
            "surroun surround\n",
            "surround surround#\n",
            "surroundi surroundin\n",
            "surroundin surrounding\n",
            "surrounding surrounding#\n",
            "surroundings surroundings#\n",
            "\n",
            " s\n",
            "d di\n",
            "da dan\n",
            "dar darr\n",
            "dark dark#\n",
            "darke darker\n",
            "darkes darkes#\n",
            "darkest darkest#\n",
            "\n",
            " s\n",
            "t th\n",
            "th thr\n",
            "tho thor\n",
            "thou thout\n",
            "\n",
            " s\n",
            "p pr\n",
            "ph pha\n",
            "phe phel\n",
            "phen phent\n",
            "pheno phenor\n",
            "phenom phenomi\n",
            "phenome phenome#\n",
            "phenomen phenoment\n",
            "phenomena phenomenat\n",
            "phenomenal phenomenal#\n",
            "phenomenall phenomenally\n",
            "phenomenally phenomenally#\n",
            "\n",
            " c\n",
            "j ju\n",
            "ju jus\n",
            "jun junc\n",
            "junc junct\n",
            "junct juncti\n",
            "juncti junctio\n",
            "junctio junction\n",
            "junction junction#\n",
            "junctions junctions#\n",
            "\n",
            " c\n",
            "b ba\n",
            "bi bil\n",
            "bio biol\n",
            "biog biogr\n",
            "biogr biogra\n",
            "biogra biograp\n",
            "biograp biograph\n",
            "biograph biographe\n",
            "biography biography#\n",
            "\n",
            " c\n",
            "f fa\n",
            "fe fen\n",
            "fee feed\n",
            "feet feet#\n",
            "\n",
            " s\n",
            "r re\n",
            "ru rus\n",
            "run rung\n",
            "rung rung#\n",
            "\n",
            " s\n",
            "d de\n",
            "di dis\n",
            "din ding\n",
            "dino dinot\n",
            "dinos dinose\n",
            "dinosa dinosal\n",
            "dinosau dinosaur\n",
            "dinosaur dinosaure\n",
            "\n",
            " s\n",
            "i in\n",
            "im imp\n",
            "imm immo\n",
            "immo immon\n",
            "immor immore\n",
            "immort immorte\n",
            "immorta immortab\n",
            "immortal immortal#\n",
            "immortali immortalis\n",
            "immortalit immortaliti\n",
            "immortality immortality#\n",
            "\n",
            " s\n",
            "p pr\n",
            "pr pro\n",
            "pre pres\n",
            "pres press\n",
            "prese presen\n",
            "presen present\n",
            "present present#\n",
            "presenta presentat\n",
            "presentat presentati\n",
            "presentati presentatio\n",
            "presentatio presentation\n",
            "presentation presentation#\n",
            "presentations presentations#\n",
            "\n",
            " c\n",
            "c co\n",
            "co con\n",
            "con cont\n",
            "cons const\n",
            "const constr\n",
            "consta constat\n",
            "constan constant\n",
            "constant constant#\n",
            "\n",
            " s\n",
            "f fo\n",
            "fe fea\n",
            "fer fere\n",
            "fert ferti\n",
            "ferti fertic\n",
            "fertil fertili\n",
            "fertili fertilit\n",
            "fertilit fertility\n",
            "fertility fertility#\n",
            "\n",
            "Epoch 2 Accuracy 0.4103 Loss tensor(23176.2852)\n",
            "Saving checkpoint\n"
          ]
        }
      ]
    }
  ]
}