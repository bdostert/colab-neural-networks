{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHMSs6kFsvKJ2ej1N/H8zy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdS5iA9lQMb3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import collections\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Sequence_as_vec:\n",
        "    def __init__(self, inventory):\n",
        "        self.inventory = inventory #A list of words of letters\n",
        "        self.vector_size = 10000\n",
        "        self.inventory_dict = collections.defaultdict()\n",
        "        for l in inventory:\n",
        "          for s in l.split():\n",
        "            self.inventory_dict[s] = 2*np.random.randint(0, 2, size=(self.vector_size))- np.ones((self.vector_size))\n",
        "            #This creates polar vectors that consist of values that are either 1 or minus 1.\n",
        "    def get_initial_zero_vector(self):\n",
        "        return np.zeros(self.vector_size)\n",
        "    def cossim(self,x,y):\n",
        "        #Cosine similarity\n",
        "        return np.dot(x,np.transpose(y))/(math.sqrt(np.dot(x,np.transpose(x)))*math.sqrt(np.dot(y,\n",
        "        np.transpose(y))))\n",
        "    def pm(self,x,offset):\n",
        "        #Permute a vector x by an offset.\n",
        "        return np.concatenate((x[offset:], x[0:offset]))\n",
        "    def ipm(self,x):\n",
        "        #Unpermute a vector one step.\n",
        "        return np.concatenate((x[-1:], x[:-1]))\n",
        "    def most_similar(self,x):\n",
        "        closest = None\n",
        "        best_cos = -1\n",
        "        for vec in self.inventory:\n",
        "            dis = self.cossim(x,self.inventory_dict[vec])\n",
        "            if dis > best_cos:\n",
        "                closest = vec\n",
        "                best_cos = dis\n",
        "        return (self.inventory_dict[closest], closest, best_cos)\n",
        "    def create_vector_for_sequence(self,sequence):\n",
        "        sequence_as_list = list(sequence)\n",
        "        #print(sequence_as_list)\n",
        "        vec = np.zeros((self.vector_size))\n",
        "        for i, l in enumerate(sequence_as_list):\n",
        "            if l in self.inventory_dict.keys():\n",
        "                vec = np.add(vec, self.pm(self.inventory_dict[l],i))\n",
        "        return vec\n",
        "    def unbind_vec(self, vec):\n",
        "        residue = vec\n",
        "        cos = -1\n",
        "        for i in range(500):\n",
        "          prev_cos = cos\n",
        "          (best_vec, closest, cos) = self.most_similar(residue)\n",
        "          if prev_cos / cos > 3:\n",
        "            break\n",
        "          print(closest, end='')\n",
        "          if cos == 1:\n",
        "            break\n",
        "          residue = self.ipm(np.subtract(residue, best_vec))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "#part 1\n",
        "chars = list(string.printable+\" \")\n",
        "a = Sequence_as_vec(chars)\n",
        "long_word = \"The quick brown fox jumps over the lazy dog\"\n",
        "print(long_word)\n",
        "temp_vec = a.create_vector_for_sequence(long_word)\n",
        "a.unbind_vec(temp_vec)\n",
        "print(temp_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfP36p_1QoJf",
        "outputId": "18bcad4f-7af1-4d8e-88d1-4d63a0ed09f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox jumps over the lazy dog\n",
            "The quick brown fox jumps over the lazy dog[-9.  3. -1. ... -1. -1. -9.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#part 2\n",
        "def get_words(book_titles):\n",
        "    sentences = []\n",
        "    book_words = []\n",
        "    for book_title in book_titles:\n",
        "        words = []\n",
        "        if os.path.isfile(book_title):\n",
        "            print('Processing file', book_title)\n",
        "            with open(book_title, 'r') as f0:\n",
        "                sentence = \"\"\n",
        "                for i, line in enumerate(f0.readlines()):\n",
        "                    if i % 1000 == 0:\n",
        "                        print('Processed', i, 'lines.')\n",
        "                    line = line.rstrip()\n",
        "                    if len(line) < 1:\n",
        "                        continue\n",
        "                    if re.search(r'^[A-Z][A-Z][A-Z]', line):\n",
        "                        continue\n",
        "                    if line[0] == '[':\n",
        "                        continue\n",
        "                    line = re.sub(r'([\\.,;:!\\?”])', r' \\1', line)\n",
        "                    line = re.sub(r'(“)', r'\\1 ', line)\n",
        "                    line = re.sub(r'[_‘]', '', line)\n",
        "                    line = re.sub('—', ' ', line)\n",
        "                    line = re.sub(r'[^a-zA-Z\\.’ ]', '', line)\n",
        "                    #print(line)\n",
        "                    buffer_empty = True\n",
        "                    lal = line.split()\n",
        "                    for wd in lal:\n",
        "                        if buffer_empty == False:\n",
        "                            sentence += wd.lower() + \" \"\n",
        "                            if wd not in words:\n",
        "                                words.append(wd.lower())\n",
        "                        buffer_empty = False\n",
        "                        if wd in ['.', '!', '?', ':', ';']:\n",
        "                            sentences.append(sentence)\n",
        "                            sentence = \"\"\n",
        "                            buffer_empty = True\n",
        "            book_words.append(words)\n",
        "        else:\n",
        "            print('No file found with  name', book_title)\n",
        "            exit()\n",
        "    return book_words, sentences\n",
        "\n",
        "book_titles = ['GreatExpectations_nll.txt', 'WizardOfOz.nnl', 'emma.txt','tale_of_2_cities_nll.txt']\n",
        "\n",
        "#part 3 and 4\n",
        "book_words, sentences = get_words(book_titles)\n",
        "seq = Sequence_as_vec(sentences)\n",
        "#part 5\n",
        "for i, bw1 in enumerate(book_words):\n",
        "  for j, bw2 in enumerate(book_words):\n",
        "    vec1 = seq.create_vector_for_sequence(bw1)\n",
        "    vec2 = seq.create_vector_for_sequence(bw2)\n",
        "    cos_sim = seq.cossim(vec1, vec2)\n",
        "    print(\"cosine similary between:\", book_titles[i], \" and \", book_titles[j], \": \", cos_sim)\n",
        "\n",
        "#part 6\n",
        "print()\n",
        "new_sentence = input(\"Enter a sentence:\")\n",
        "best_cos, best_book = 0, 0\n",
        "for idx, bw in enumerate(book_words):\n",
        "  new_vec = seq.create_vector_for_sequence(new_sentence)\n",
        "  vec = seq.create_vector_for_sequence(bw)\n",
        "  if seq.cossim(new_vec, vec) > best_cos:\n",
        "    best_cos = seq.cossim(new_vec, vec)\n",
        "    best_book = idx\n",
        "print(\"Closet book to input sentence is: \", book_titles[best_book], \" with cosine similarity of: \", best_cos)\n",
        "\n"
      ],
      "metadata": {
        "id": "eOCEjAx2QsP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc76da3e-ccb1-486f-9541-3ed68cebe1a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file GreatExpectations_nll.txt\n",
            "Processed 0 lines.\n",
            "Processed 1000 lines.\n",
            "Processed 2000 lines.\n",
            "Processed 3000 lines.\n",
            "Processed 4000 lines.\n",
            "Processing file WizardOfOz.nnl\n",
            "Processed 0 lines.\n",
            "Processed 1000 lines.\n",
            "Processed 2000 lines.\n",
            "Processing file emma.txt\n",
            "Processed 0 lines.\n",
            "Processed 1000 lines.\n",
            "Processed 2000 lines.\n",
            "Processing file tale_of_2_cities_nll.txt\n",
            "Processed 0 lines.\n",
            "Processed 1000 lines.\n",
            "Processed 2000 lines.\n",
            "Processed 3000 lines.\n",
            "cosine similary between: GreatExpectations_nll.txt  and  GreatExpectations_nll.txt :  1.0\n",
            "cosine similary between: GreatExpectations_nll.txt  and  WizardOfOz.nnl :  0.008942251836429292\n",
            "cosine similary between: GreatExpectations_nll.txt  and  emma.txt :  0.8881185667214301\n",
            "cosine similary between: GreatExpectations_nll.txt  and  tale_of_2_cities_nll.txt :  0.8791174111190005\n",
            "cosine similary between: WizardOfOz.nnl  and  GreatExpectations_nll.txt :  0.008942251836429292\n",
            "cosine similary between: WizardOfOz.nnl  and  WizardOfOz.nnl :  1.0000000000000002\n",
            "cosine similary between: WizardOfOz.nnl  and  emma.txt :  0.013381199549989283\n",
            "cosine similary between: WizardOfOz.nnl  and  tale_of_2_cities_nll.txt :  0.01118839533067672\n",
            "cosine similary between: emma.txt  and  GreatExpectations_nll.txt :  0.8881185667214301\n",
            "cosine similary between: emma.txt  and  WizardOfOz.nnl :  0.013381199549989283\n",
            "cosine similary between: emma.txt  and  emma.txt :  1.0000000000000002\n",
            "cosine similary between: emma.txt  and  tale_of_2_cities_nll.txt :  0.8413169439543658\n",
            "cosine similary between: tale_of_2_cities_nll.txt  and  GreatExpectations_nll.txt :  0.8791174111190005\n",
            "cosine similary between: tale_of_2_cities_nll.txt  and  WizardOfOz.nnl :  0.01118839533067672\n",
            "cosine similary between: tale_of_2_cities_nll.txt  and  emma.txt :  0.8413169439543658\n",
            "cosine similary between: tale_of_2_cities_nll.txt  and  tale_of_2_cities_nll.txt :  1.0\n",
            "\n",
            "Enter a sentence:Emma Woodhouse, handsome, clever and rich, with a comfortable home and happy disposition seemed to unite some of the best blessings of existence and had lived nearly twenty-one years in the world with very little to distress or vex her.\n",
            "Closet book to input sentence is:  emma.txt  with cosine similarity of:  0.003274023858182772\n"
          ]
        }
      ]
    }
  ]
}